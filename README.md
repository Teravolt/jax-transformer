# Transformer implemented using JAX

This repo contains code for an old project I started back in 2021 for implementing a Transformer model from scratch using JAX.
I started this project originally to challenge myself to code a neural network architecture from scratch and to learn JAX.

## Installation

The JAX Transformer has been tested using Python `3.9`.
If you are able to get this running on an older version of Python, or the Transformer fails to run on a later version, please open an issue and I will look into it.

You can set up your Python envrionment using any method (e.g., Poetry, pipenv, conda, etc.), but please
make sure you have **all** packages from `requirements.txt` installed. 

**NOTE**: I have tested this with `pipenv` and included the relevant `pipenv` files.
If you have success with other methods, please let me know and I can add instructions here!

Once you have all necessary packages installed, you can train and evaluate the Transformer.

## Training

TODO: Write training instructions

## Evaluation

TODO: Write evaluation instructions

## Future Work